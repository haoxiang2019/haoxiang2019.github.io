<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="深藏功与名">
<meta property="og:url" content="http://smoba.cn/index.html">
<meta property="og:site_name" content="深藏功与名">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深藏功与名">






  <link rel="canonical" href="http://smoba.cn/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>深藏功与名</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">深藏功与名</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://smoba.cn/2019/02/19/Anomaly-Detection-Learning-Resources/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Haoxiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深藏功与名">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/19/Anomaly-Detection-Learning-Resources/" class="post-title-link" itemprop="url">Anomaly Detection Learning Resources</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-19 12:06:14 / 修改时间：12:49:39" itemprop="dateCreated datePublished" datetime="2019-02-19T12:06:14+08:00">2019-02-19</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/异常检测/" itemprop="url" rel="index"><span itemprop="name">异常检测</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong><em><a href="https://en.wikipedia.org/wiki/Anomaly_detection" target="_blank" rel="noopener">Outlier Detection</a></em></strong> (also known as <strong><em><a href="https://en.wikipedia.org/wiki/Anomaly_detection" target="_blank" rel="noopener">Anomaly Detection</a></em></strong> ) is a fascinating and useful technique to identify outlying data objects. It has been proven critical in many fields, such as credit card fraud analytics and mechanical unit defect detection.</p>
<p>In this repository, you could find:</p>
<ol>
<li>Books &amp; Academic Papers </li>
<li>Learning Materials, e.g., online courses and videos </li>
<li>Outlier Datasets</li>
<li>Open-source Libraries &amp; Demo Codes</li>
<li><strong>Paper Downloader</strong>: a Python 3 script to download the open access papers listed in this repository (under development). </li>
</ol>
<h2 id="1-Books-amp-Tutorials"><a href="#1-Books-amp-Tutorials" class="headerlink" title="1. Books &amp; Tutorials"></a>1. Books &amp; Tutorials</h2><h3 id="1-1-Books"><a href="#1-1-Books" class="headerlink" title="1.1. Books"></a>1.1. Books</h3><p><a href="https://www.springer.com/gp/book/9781461463955" target="_blank" rel="noopener">Outlier Analysis</a> by Charu Aggarwal: Classical text book covering most of the outlier analysis techniques.<br>A must-read for people in outlier detection. <a href="http://charuaggarwal.net/outlierbook.pdf" target="_blank" rel="noopener">[Preview.pdf]</a></p>
<p><a href="https://www.springer.com/gp/book/9783319547640" target="_blank" rel="noopener">Outlier Ensembles: An Introduction</a> by Charu Aggarwal and Saket Sathe: Great intro book for ensemble learning in outlier analysis.</p>
<p><a href="https://www.elsevier.com/books/data-mining-concepts-and-techniques/han/978-0-12-381479-1" target="_blank" rel="noopener">Data Mining: Concepts and Techniques (3rd)</a> by Jiawei Han Micheline Kamber Jian Pei: Chapter 12 discusses outlier detection with many important points. <a href="https://www.google.ca/search?&amp;q=data+mining+jiawei+han&amp;oq=data+ming+jiawei" target="_blank" rel="noopener">[Google Search]</a></p>
<h3 id="1-2-Tutorials"><a href="#1-2-Tutorials" class="headerlink" title="1.2. Tutorials"></a>1.2. Tutorials</h3><p>Kriegel, H.P., Kröger, P. and Zimek, A., 2010. Outlier detection techniques. <em>Tutorial at ACM SIGKDD</em>, 10. <a href="https://imada.sdu.dk/~zimek/publications/KDD2010/kdd10-outlier-tutorial.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Chawla, S. and Chandola, V., 2011, Anomaly Detection: A Tutorial. <em>Tutorial at ICDM 2011</em>.  <a href="http://webdocs.cs.ualberta.ca/~icdm2011/downloads/ICDM2011_anomaly_detection_tutorial.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Lazarevic, A., Banerjee, A., Chandola, V., Kumar, V. and Srivastava, J., 2008, September. Data mining for anomaly detection. In <em>Tutorial at ECML PKDD 2008</em>. <a href="http://videolectures.net/ecmlpkdd08_lazarevic_dmfa/" target="_blank" rel="noopener">[See Video]</a></p>
<h2 id="2-Courses-Seminars-Videos"><a href="#2-Courses-Seminars-Videos" class="headerlink" title="2. Courses/Seminars/Videos"></a>2. Courses/Seminars/Videos</h2><p><strong>Coursera Introduction to Anomaly Detection (by IBM)</strong>:<br><a href="https://www.coursera.org/learn/ai/lecture/ASPv0/introduction-to-anomaly-detection" target="_blank" rel="noopener">https://www.coursera.org/learn/ai/lecture/ASPv0/introduction-to-anomaly-detection</a></p>
<p><strong>Coursera Machine Learning by Andrew Ng also partly covers the topic</strong>:</p>
<ul>
<li><a href="https://www.coursera.org/learn/machine-learning/lecture/Rkc5x/anomaly-detection-vs-supervised-learning" target="_blank" rel="noopener">Anomaly Detection vs. Supervised Learning</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning/lecture/Mwrni/developing-and-evaluating-an-anomaly-detection-system" target="_blank" rel="noopener">Developing and Evaluating an Anomaly Detection System</a></li>
</ul>
<p><strong>Udemy Outlier Detection Algorithms in Data Mining and Data Science</strong>: <a href="https://www.udemy.com/outlier-detection-techniques/" target="_blank" rel="noopener">https://www.udemy.com/outlier-detection-techniques/</a></p>
<p><strong>Stanford Data Mining for Cyber Security</strong> also covers part of anomaly detection techniques. <a href="http://web.stanford.edu/class/cs259d/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs259d/</a></p>
<h2 id="3-Toolbox-amp-Datasets"><a href="#3-Toolbox-amp-Datasets" class="headerlink" title="3. Toolbox &amp; Datasets"></a>3. Toolbox &amp; Datasets</h2><h3 id="3-1-Python"><a href="#3-1-Python" class="headerlink" title="3.1. Python"></a>3.1. Python</h3><p><a href="http://scikit-learn.org/stable/modules/outlier_detection.html" target="_blank" rel="noopener">Scikit-learn Novelty and Outlier Detection</a>. It supports some popular algorithms like LOF, Isolation Forest and One-class SVM</p>
<p><a href="https://github.com/yzhao062/Pyod" target="_blank" rel="noopener">Python Outlier Detection (PyOD)</a>: It supports a series of outlier detection algorithms and combination frameworks. It is now released on PyPI and can be installed with “pip install pyod”.</p>
<h3 id="3-2-Matlab"><a href="#3-2-Matlab" class="headerlink" title="3.2. Matlab"></a>3.2. Matlab</h3><p><a href="http://dsmi-lab-ntust.github.io/AnomalyDetectionToolbox/" target="_blank" rel="noopener">Anomaly Detection Toolbox - Beta</a>: A collection of popular outlier detection algorithms in Matlab.</p>
<h3 id="3-3-Java"><a href="#3-3-Java" class="headerlink" title="3.3. Java"></a>3.3. Java</h3><p><a href="https://elki-project.github.io/" target="_blank" rel="noopener">ELKI: Environment for Developing KDD-Applications Supported by Index-Structures</a>:<br>ELKI is an open source (AGPLv3) data mining software written in Java. The focus of ELKI is research in algorithms, with an emphasis on unsupervised methods in cluster analysis and outlier detection. </p>
<p><a href="https://github.com/Markus-Go/rapidminer-anomalydetection" target="_blank" rel="noopener">RapidMiner Anomaly Detection Extension</a>: The Anomaly Detection Extension for RapidMiner comprises the most well know unsupervised anomaly detection algorithms, assigning individual anomaly scores to data rows of example sets. It allows you to find data, which is significantly different from the normal, without the need for the data being labeled.</p>
<h3 id="3-4-Time-series-outlier-detection"><a href="#3-4-Time-series-outlier-detection" class="headerlink" title="3.4. Time series outlier detection"></a>3.4. Time series outlier detection</h3><ul>
<li><a href="https://github.com/MentatInnovations/datastream.io" target="_blank" rel="noopener">datastream.io</a></li>
<li><a href="https://github.com/earthgecko/skyline" target="_blank" rel="noopener">skyline</a></li>
<li><a href="https://github.com/tsurubee/banpei" target="_blank" rel="noopener">banpei</a></li>
<li><a href="https://github.com/twitter/AnomalyDetection" target="_blank" rel="noopener">AnomalyDetection</a></li>
</ul>
<h3 id="3-5-Datasets"><a href="#3-5-Datasets" class="headerlink" title="3.5. Datasets"></a>3.5. Datasets</h3><p><strong>ELKI Outlier Datasets</strong>: <a href="https://elki-project.github.io/datasets/outlier" target="_blank" rel="noopener">https://elki-project.github.io/datasets/outlier</a></p>
<p><strong>Outlier Detection DataSets (ODDS)</strong>: <a href="http://odds.cs.stonybrook.edu/#table1" target="_blank" rel="noopener">http://odds.cs.stonybrook.edu/#table1</a></p>
<p><strong>Unsupervised Anomaly Detection Dataverse</strong>: <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OPQMVF" target="_blank" rel="noopener">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OPQMVF</a></p>
<p><strong>Anomaly Detection Meta-Analysis Benchmarks</strong>: <a href="https://ir.library.oregonstate.edu/concern/datasets/47429f155" target="_blank" rel="noopener">https://ir.library.oregonstate.edu/concern/datasets/47429f155</a></p>
<h2 id="4-Papers"><a href="#4-Papers" class="headerlink" title="4. Papers"></a>4. Papers</h2><h3 id="4-1-Overview-amp-Survey-Papers"><a href="#4-1-Overview-amp-Survey-Papers" class="headerlink" title="4.1. Overview &amp; Survey Papers"></a>4.1. Overview &amp; Survey Papers</h3><p>Chandola, V., Banerjee, A. and Kumar, V., 2009. Anomaly detection: A survey. <em>ACM computing surveys</em> , 41(3), p.15. <a href="https://www.vs.inf.ethz.ch/edu/HS2011/CPS/papers/chandola09_anomaly-detection-survey.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Hodge, V. and Austin, J., 2004. A survey of outlier detection methodologies. <em>Artificial intelligence review</em>, 22(2), pp.85-126. <a href="https://www-users.cs.york.ac.uk/vicky/myPapers/Hodge+Austin_OutlierDetection_AIRE381.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Campos, G.O., Zimek, A., Sander, J., Campello, R.J., Micenková, B., Schubert, E., Assent, I. and Houle, M.E., 2016. On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. <em>Data Mining and Knowledge Discovery</em>, 30(4), pp.891-927. <a href="https://link.springer.com/article/10.1007/s10618-015-0444-8" target="_blank" rel="noopener">[HTML]</a><br><a href="https://imada.sdu.dk/~zimek/InvitedTalks/TUVienna-2016-05-18-outlier-evaluation.pdf" target="_blank" rel="noopener">[SLIDES]</a></p>
<p>Singh, K., &amp; Upadhyaya, S. (2012). Outlier detection: applications and techniques. International Journal of Computer Science Issues (IJCSI), 9(1), 307. <a href="https://pdfs.semanticscholar.org/4f58/44c9e7db68af7c2c5b918082636c3307cef9.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Goldstein, M. and Uchida, S., 2016. A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data. <em>PloS one</em>, 11(4), p.e0152173.  <a href="http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0152173&amp;type=printable" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-2-Key-Algorithms"><a href="#4-2-Key-Algorithms" class="headerlink" title="4.2. Key Algorithms"></a>4.2. Key Algorithms</h3><p><strong>k Nearst Neighbors (kNN) Outlier Detector</strong>.</p>
<ul>
<li>Ramaswamy, S., Rastogi, R. and Shim, K., 2000, May. Efficient algorithms for mining outliers from large data sets. <em>ACM Sigmod Record</em>, 29(2), pp. 427-438). <a href="https://webdocs.cs.ualberta.ca/~zaiane/pub/check/ramaswamy.pdf" target="_blank" rel="noopener">[Download PDF]</a></li>
<li>Angiulli, F. and Pizzuti, C., 2002, August. Fast outlier detection in high dimensional spaces. In <em>European Conference on Principles of Data Mining and Knowledge Discovery</em> pp. 15-27. <a href="https://link.springer.com/chapter/10.1007/3-540-45681-3_2" target="_blank" rel="noopener">[HTML]</a></li>
</ul>
<p><strong>Local Outlier Factor (LOF)</strong>. Breunig, M.M., Kriegel, H.P., Ng, R.T. and Sander, J., 2000, May. LOF: identifying density-based local outliers. <em>ACM Sigmod Record</em>, 29(2), pp. 93-104. <a href="http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p><strong>Isolation Forest</strong>. Liu, F.T., Ting, K.M. and Zhou, Z.H., 2008, December. Isolation forest. In <em>ICDM ‘08</em>, pp. 413-422. IEEE. <a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p><strong>One-class Support Vector Machine</strong>. Ma, J. and Perkins, S., 2003, July. Time-series novelty detection using one-class support vector machines. In <em>IJCNN’ 03</em>, pp. 1741-1745. IEEE. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.653.2440&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-3-Graph-amp-Network-Outlier-Detection"><a href="#4-3-Graph-amp-Network-Outlier-Detection" class="headerlink" title="4.3. Graph &amp; Network Outlier Detection"></a>4.3. Graph &amp; Network Outlier Detection</h3><p>Akoglu, L., Tong, H. and Koutra, D., 2015. Graph based anomaly detection and description: a survey. <em>Data Mining and Knowledge Discovery</em>, 29(3), pp.626-688. <a href="https://arxiv.org/pdf/1404.4679.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Ranshous, S., Shen, S., Koutra, D., Harenberg, S., Faloutsos, C. and Samatova, N.F., 2015. Anomaly detection in dynamic networks: a survey. Wiley Interdisciplinary Reviews: Computational Statistics, 7(3), pp.223-247. <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1347" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-4-Time-Series-Outlier-Detection"><a href="#4-4-Time-Series-Outlier-Detection" class="headerlink" title="4.4. Time Series Outlier Detection"></a>4.4. Time Series Outlier Detection</h3><p>Gupta, M., Gao, J., Aggarwal, C.C. and Han, J., 2014. Outlier detection for temporal data: A survey. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 26(9), pp.2250-2267. <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2014/01/gupta14_tkde.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-5-Feature-Selection-in-Outlier-Detection"><a href="#4-5-Feature-Selection-in-Outlier-Detection" class="headerlink" title="4.5. Feature Selection in Outlier Detection"></a>4.5. Feature Selection in Outlier Detection</h3><p>Pang, G., Cao, L., Chen, L. and Liu, H., 2016, December. Unsupervised feature selection for outlier detection by modelling hierarchical value-feature couplings. In Data Mining (ICDM), 2016 IEEE 16th International Conference on (pp. 410-419). IEEE. <a href="https://opus.lib.uts.edu.au/bitstream/10453/107356/4/DSFS_ICDM2016.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Pang, G., Cao, L., Chen, L. and Liu, H., 2017, August. Learning homophily couplings from non-iid data for joint feature selection and noise-resilient outlier detection. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (pp. 2585-2591). AAAI Press. <a href="https://www.ijcai.org/proceedings/2017/0360.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-6-High-dimensional-amp-Subspace-Outliers"><a href="#4-6-High-dimensional-amp-Subspace-Outliers" class="headerlink" title="4.6. High-dimensional &amp; Subspace Outliers"></a>4.6. High-dimensional &amp; Subspace Outliers</h3><p>Zimek, A., Schubert, E. and Kriegel, H.P., 2012. A survey on unsupervised outlier detection in high‐dimensional numerical data. <em>Statistical Analysis and Data Mining: The ASA Data Science Journal</em>, 5(5), pp.363-387. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11161" target="_blank" rel="noopener">[Downloadable Link]</a></p>
<p>Pang, G., Cao, L., Chen, L. and Liu, H., 2018. Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection. In <em>24th ACM SIGKDD International Conference on Knowledge Discovery and Data mining (KDD)</em>. 2018. <a href="https://arxiv.org/pdf/1806.04808.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-7-Outlier-Ensembles"><a href="#4-7-Outlier-Ensembles" class="headerlink" title="4.7. Outlier Ensembles"></a>4.7. Outlier Ensembles</h3><p>Aggarwal, C.C., 2013. Outlier ensembles: position paper. <em>ACM SIGKDD Explorations Newsletter</em>, 14(2), pp.49-58. <a href="https://pdfs.semanticscholar.org/841e/ce7c3812bbf799c99c84c064bbcf77916ba9.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Zimek, A., Campello, R.J. and Sander, J., 2014. Ensembles for unsupervised outlier detection: challenges and research questions a position paper. <em>ACM Sigkdd Explorations Newsletter</em>, 15(1), pp.11-22. <a href="http://www.kdd.org/exploration_files/V15-01-02-Zimek.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Campos, G.O., Zimek, A. and Meira, W., 2018, June. An Unsupervised Boosting Strategy for Outlier Detection Ensembles. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 564-576). Springer, Cham. <a href="https://link.springer.com/chapter/10.1007/978-3-319-93034-3_45" target="_blank" rel="noopener">[HTML]</a></p>
<h3 id="4-8-Outlier-Detection-in-Evolving-Data"><a href="#4-8-Outlier-Detection-in-Evolving-Data" class="headerlink" title="4.8. Outlier Detection in Evolving Data"></a>4.8. Outlier Detection in Evolving Data</h3><p>Salehi, Mahsa &amp; Rashidi, Lida. (2018). A Survey on Anomaly detection in Evolving Data: [with Application to Forest Fire Risk Prediction]. <em>ACM SIGKDD Explorations Newsletter</em>. 20. 13-23. <a href="http://www.kdd.org/exploration_files/20-1-Article2.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Emaad Manzoor, Hemank Lamba, Leman Akoglu. Outlier Detection in Feature-Evolving Data Streams. In <em>24th ACM SIGKDD International Conference on Knowledge Discovery and Data mining (KDD)</em>. 2018. <a href="https://www.andrew.cmu.edu/user/lakoglu/pubs/18-kdd-xstream.pdf" target="_blank" rel="noopener">[Download PDF]</a><br><a href="https://cmuxstream.github.io/" target="_blank" rel="noopener">[Github]</a></p>
<h3 id="4-9-Representation-Learning-in-Outlier-Detection"><a href="#4-9-Representation-Learning-in-Outlier-Detection" class="headerlink" title="4.9. Representation Learning in Outlier Detection"></a>4.9. Representation Learning in Outlier Detection</h3><p>Pang, G., Cao, L., Chen, L. and Liu, H., 2018. Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection. In <em>24th ACM SIGKDD International Conference on Knowledge Discovery and Data mining (KDD)</em>. 2018. <a href="https://arxiv.org/pdf/1806.04808.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Micenková, B., McWilliams, B. and Assent, I., 2015. Learning representations for outlier detection on a budget. arXiv preprint arXiv:1507.08104. <a href="https://arxiv.org/pdf/1507.08104.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Zhao, Y., Hryniewicki, M.K. and PricewaterhouseCoopers, A., 2018. XGBOD: Improving Supervised Outlier Detection with Unsupervised Representation Learning. <em>International Joint Conference on Neural Networks</em>. <a href="https://www.cs.toronto.edu/~yuezhao/s/edited_XGBOD.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-10-Interpretability"><a href="#4-10-Interpretability" class="headerlink" title="4.10. Interpretability"></a>4.10. Interpretability</h3><p>Nikhil Gupta, Dhivya Eswaran, Neil Shah, Leman Akoglu, Christos Faloutsos. Beyond Outlier Detection: LookOut for Pictorial Explanation. <em>ECML PKDD 2018</em>. <a href="https://www.andrew.cmu.edu/user/lakoglu/pubs/18-pkdd-lookout.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Liu, N., Shin, D. and Hu, X., 2017. Contextual outlier interpretation. arXiv preprint arXiv:1711.10589. <a href="https://arxiv.org/pdf/1711.10589.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Tang, G., Pei, J., Bailey, J. and Dong, G., 2015. Mining multidimensional contextual outliers from categorical relational data. Intelligent Data Analysis, 19(5), pp.1171-1192.  <a href="http://www.cs.sfu.ca/~jpei/publications/Contextual%20outliers.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Dang, X.H., Assent, I., Ng, R.T., Zimek, A. and Schubert, E., 2014, March. Discriminative features for identifying and interpreting outliers. In <em>International Conference on Data Engineering (ICDE)</em>. IEEE. <a href="http://cs.au.dk/~dang/icde2014.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-11-Social-Media-Anomaly-Detection"><a href="#4-11-Social-Media-Anomaly-Detection" class="headerlink" title="4.11. Social Media Anomaly Detection"></a>4.11. Social Media Anomaly Detection</h3><p>Yu, R., Qiu, H., Wen, Z., Lin, C. and Liu, Y., 2016. A survey on social media anomaly detection. <em>ACM SIGKDD Explorations Newsletter</em>, 18(1), pp.1-14. <a href="https://arxiv.org/pdf/1601.01102.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<p>Yu, R., He, X. and Liu, Y., 2015. Glad: group anomaly detection in social media analysis. <em>ACM Transactions on Knowledge Discovery from Data (TKDD)</em>, 10(2), p.18. <a href="https://arxiv.org/pdf/1410.1940.pdf" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-12-Outlier-Detection-in-Other-fields"><a href="#4-12-Outlier-Detection-in-Other-fields" class="headerlink" title="4.12. Outlier Detection in Other fields"></a>4.12. Outlier Detection in Other fields</h3><p>Kannan, R., Woo, H., Aggarwal, C.C. and Park, H., 2017, June. Outlier detection for text data. In Proceedings of the 2017 SIAM International Conference on Data Mining (pp. 489-497). Society for Industrial and Applied Mathematics. <a href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611974973.55" target="_blank" rel="noopener">[Download PDF]</a></p>
<h3 id="4-13-Outlier-Detection-Applications"><a href="#4-13-Outlier-Detection-Applications" class="headerlink" title="4.13. Outlier Detection Applications"></a>4.13. Outlier Detection Applications</h3><p><strong>Security</strong>:</p>
<ul>
<li>Weller-Fahy, D.J., Borghetti, B.J. and Sodemann, A.A., 2015. A survey of distance and similarity measures used within network intrusion anomaly detection. <em>IEEE Communications Surveys &amp; Tutorials</em>, 17(1), pp.70-91. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853338" target="_blank" rel="noopener">[Download PDF]</a></li>
<li>Garcia-Teodoro, P., Diaz-Verdejo, J., Maciá-Fernández, G. and Vázquez, E., 2009. Anomaly-based network intrusion detection: Techniques, systems and challenges. <em>computers &amp; security</em>, 28(1-2), pp.18-28. <a href="http://dtstc.ugr.es/~jedv/descargas/2009_CoSe09-Anomaly-based-network-intrusion-detection-Techniques,-systems-and-challenges.pdf" target="_blank" rel="noopener">[Download PDF]</a></li>
</ul>
<p><strong>Finance</strong>:</p>
<ul>
<li>Ahmed, M., Mahmood, A.N. and Islam, M.R., 2016. A survey of anomaly detection techniques in financial domain. Future Generation Computer Systems, 55, pp.278-288. <a href="http://isiarticles.com/bundles/Article/pre/pdf/76882.pdf" target="_blank" rel="noopener">[Download PDF]</a></li>
</ul>
<h2 id="5-Key-Conferences-Workshops-Journals"><a href="#5-Key-Conferences-Workshops-Journals" class="headerlink" title="5. Key Conferences/Workshops/Journals"></a>5. Key Conferences/Workshops/Journals</h2><h3 id="5-1-Conferences-amp-Workshops"><a href="#5-1-Conferences-amp-Workshops" class="headerlink" title="5.1. Conferences &amp; Workshops"></a>5.1. Conferences &amp; Workshops</h3><p><a href="http://www.kdd.org/conferences" target="_blank" rel="noopener">ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</a><br> <strong>Note: SIGKDD usually has an Outlier Detection Workshop (ODD), see <a href="https://www.andrew.cmu.edu/user/lakoglu/odd/index.html" target="_blank" rel="noopener">ODD 2018</a></strong>.</p>
<p><a href="https://sigmod.org/" target="_blank" rel="noopener">ACM International Conference on Management of Data (SIGMOD)</a></p>
<p><a href="https://www2018.thewebconf.org/" target="_blank" rel="noopener">The Web Conference (WWW)</a></p>
<p><a href="http://icdm2018.org/" target="_blank" rel="noopener">IEEE International Conference on Data Mining (ICDM)</a></p>
<p><a href="https://www.siam.org/Conferences/CM/Main/sdm19" target="_blank" rel="noopener">SIAM International Conference on Data Mining (SDM)</a></p>
<p><a href="https://icde2018.org/" target="_blank" rel="noopener">IEEE International Conference on Data Engineering (ICDE)</a></p>
<p><a href="http://www.cikmconference.org/" target="_blank" rel="noopener">ACM InternationalConference on Information and Knowledge Management (CIKM)</a></p>
<p><a href="http://www.wsdm-conference.org/2018/" target="_blank" rel="noopener">ACM International Conference on Web Search and Data Mining (WSDM)</a></p>
<p><a href="http://www.ecmlpkdd2018.org/" target="_blank" rel="noopener">The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)</a></p>
<p><a href="http://pakdd2019.medmeeting.org" target="_blank" rel="noopener">The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</a></p>
<h3 id="5-2-Journals"><a href="#5-2-Journals" class="headerlink" title="5.2. Journals"></a>5.2. Journals</h3><p><a href="https://tkdd.acm.org/" target="_blank" rel="noopener">ACM Transactions on Knowledge Discovery from Data (TKDD)</a></p>
<p><a href="https://www.computer.org/web/tkde" target="_blank" rel="noopener">IEEE Transactions on Knowledge and Data Engineering (TKDE)</a></p>
<p><a href="http://www.kdd.org/explorations" target="_blank" rel="noopener">ACM SIGKDD Explorations Newsletter</a></p>
<p><a href="https://link.springer.com/journal/10618" target="_blank" rel="noopener">Data Mining and Knowledge Discovery</a></p>
<p><a href="https://link.springer.com/journal/10115" target="_blank" rel="noopener">Knowledge and Information Systems (KAIS)</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://smoba.cn/2019/01/28/Day-11-K-Nearest-Neighbors/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Haoxiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深藏功与名">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/28/Day-11-K-Nearest-Neighbors/" class="post-title-link" itemprop="url">Day 011 | K-近邻算法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-28 19:48:59" itemprop="dateCreated datePublished" datetime="2019-01-28T19:48:59+08:00">2019-01-28</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-29 14:10:57" itemprop="dateModified" datetime="2019-01-29T14:10:57+08:00">2019-01-29</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="K-近邻算法"><a href="#K-近邻算法" class="headerlink" title="K-近邻算法"></a>K-近邻算法</h2><h3 id="什么是k-NN？"><a href="#什么是k-NN？" class="headerlink" title="什么是k-NN？"></a>什么是k-NN？</h3><p>k-近邻算法是一种简单且实用的分类算法，它也可以用来做回归。</p>
<p>k-NN是无参数学习的（也就是对底层数据分布不会做任何假设），基于实例的（也就是没有显式地学习模型，而是记住训练实例），有监督的学习算法。</p>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>为了对未分类的对象进行分类，先计算该对象跟已标记对象之间的距离，然后在k个最近的节点分类中，选择最多的那个分类。在实际中，最常用的距离度量是欧氏距离。</p>
<h3 id="k的取值"><a href="#k的取值" class="headerlink" title="k的取值"></a>k的取值</h3><p>找到合理的k值并不容易。取太小的话，误差比较大；取太大的话，计算成本比较高。这很大程度上取决于实际情况，有些情况下可以遍历每个可能的k值，然后再选择。</p>
<h3 id="如何工作"><a href="#如何工作" class="headerlink" title="如何工作"></a>如何工作</h3><p>k-NN用于分类时，输出是一个类别。该算法包含三大要素：</p>
<ol>
<li>一组标记好的对象；</li>
<li>对象之间的距离；</li>
<li>k值：最近邻数。</li>
</ol>
<h3 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h3><p>常见的距离算法：</p>
<ol>
<li><p>欧氏距离</p>
<p>$D_{x, y} = \sqrt{\sum(x_i - y_i) ^ 2 }$</p>
</li>
<li><p>汉明距离</p>
<p>$D_{x,y} = \sum x_i \bigoplus y_i​$</p>
</li>
<li><p>曼哈顿距离</p>
<p>$D_{x,y} = \sum \vert{x_i - y_i}\vert$</p>
</li>
<li><p>切比雪夫距离</p>
<p>$D_{x,y} = max(\vert{x_i - y_i}\vert)​$</p>
</li>
<li><p>闵可夫斯基距离</p>
<p>$D_{x, y} = \sqrt[p]{\sum\vert{x_i - y_i}\vert ^ p }$</p>
</li>
<li><p>标准化欧氏距离</p>
<p>$D_{x, y} = \sqrt{\sum(\frac{x_i - y_i}{S_k}) ^ 2 }$</p>
</li>
<li><p><a href="https://my.oschina.net/hunglish/blog/787596" target="_blank" rel="noopener">其他</a></p>
</li>
</ol>
<h2 id="数据集：社交网络"><a href="#数据集：社交网络" class="headerlink" title="数据集：社交网络"></a>数据集：社交网络</h2><p><img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/raw/master/Other%20Docs/data.PNG" alt="img"></p>
<h2 id="Step-1-数据预处理"><a href="#Step-1-数据预处理" class="headerlink" title="Step 1: 数据预处理"></a>Step 1: 数据预处理</h2><h3 id="导入所需的库"><a href="#导入所需的库" class="headerlink" title="导入所需的库"></a>导入所需的库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h3 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h3><p><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/datasets/Social_Network_Ads.csv" target="_blank" rel="noopener">Social_Network_Ads.csv</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">'../datasets/Social_Network_Ads.csv'</span>)</span><br><span class="line">X = dataset.iloc[:, [<span class="number">2</span>, <span class="number">3</span>]].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">-1</span>].values</span><br></pre></td></tr></table></figure>
<h3 id="拆分数据集"><a href="#拆分数据集" class="headerlink" title="拆分数据集"></a>拆分数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.25</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">sc = StandardScaler()</span><br><span class="line">X_train = sc.fit_transform(X_train)</span><br><span class="line">X_test = sc.transform(X_test)</span><br></pre></td></tr></table></figure>
<h2 id="Step-2-k-NN分类模型"><a href="#Step-2-k-NN分类模型" class="headerlink" title="Step 2: k-NN分类模型"></a>Step 2: k-NN分类模型</h2><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">classifier = KNeighborsClassifier(n_neighbors=<span class="number">5</span>, metric=<span class="string">'minkowski'</span>, p=<span class="number">2</span>)</span><br><span class="line">classifier.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h2 id="Step-3-预测"><a href="#Step-3-预测" class="headerlink" title="Step 3: 预测"></a>Step 3: 预测</h2>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = classifier.predict(X_test)</span><br></pre></td></tr></table></figure>
<h2 id="Step-4-评估预测结果"><a href="#Step-4-评估预测结果" class="headerlink" title="Step 4: 评估预测结果"></a>Step 4: 评估预测结果</h2><h3 id="制作混淆矩阵"><a href="#制作混淆矩阵" class="headerlink" title="制作混淆矩阵"></a>制作混淆矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://smoba.cn/2019/01/24/Day-6-Logistic-Regression/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Haoxiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深藏功与名">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/24/Day-6-Logistic-Regression/" class="post-title-link" itemprop="url">Day 006 | 逻辑回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-24 10:47:42" itemprop="dateCreated datePublished" datetime="2019-01-24T10:47:42+08:00">2019-01-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-29 14:09:03" itemprop="dateModified" datetime="2019-01-29T14:09:03+08:00">2019-01-29</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><h3 id="什么是逻辑回归？"><a href="#什么是逻辑回归？" class="headerlink" title="什么是逻辑回归？"></a>什么是逻辑回归？</h3><p>逻辑回归是用来做0/1分类的，比如，判断一个人会不会对即将到来的选举进行投票。</p>
<h3 id="如何工作？"><a href="#如何工作？" class="headerlink" title="如何工作？"></a>如何工作？</h3><p>逻辑回归使用基础逻辑函数，根据自变量计算预测结果的概率。</p>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>在实际预测中，将概率根据阈值分类器转化成0/1。</p>
<h3 id="逻辑回归-vs-线性回归"><a href="#逻辑回归-vs-线性回归" class="headerlink" title="逻辑回归 vs 线性回归"></a>逻辑回归 vs 线性回归</h3><p>逻辑回归结果是离散的，线性回归结果是连续的。</p>
<h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><p>Sigmoid函数是一个S型曲线，可以将任意实数转化为0~1之间的值。</p>
<p>$\phi(z) = \frac {1} {1 + e ^ {-z}}$</p>
<h2 id="数据集：社交网络"><a href="#数据集：社交网络" class="headerlink" title="数据集：社交网络"></a>数据集：社交网络</h2><p><img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/raw/master/Other%20Docs/data.PNG" alt="img"></p>
<p>该数据集包含社交网络里的用户信息：用户id、性别、年龄、收入。一家汽车公司刚刚推出了他们全新的豪华SUV，我们观察其中哪些人会购买，购买结果在用户信息的最后一列。然后据此建立一个模型，来根据一个用户年龄和收入这两个维度去预测他是否会购买，也就是找出用户的年龄、收入与他是否决定购买新款SUV之间的相关性。</p>
<h2 id="Step-1-数据预处理"><a href="#Step-1-数据预处理" class="headerlink" title="Step 1: 数据预处理"></a>Step 1: 数据预处理</h2><h3 id="导入所需的库"><a href="#导入所需的库" class="headerlink" title="导入所需的库"></a>导入所需的库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h3 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h3><p><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/datasets/Social_Network_Ads.csv" target="_blank" rel="noopener">Social_Network_Ads.csv</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">'../datasets/Social_Network_Ads.csv'</span>)</span><br><span class="line">X = dataset.iloc[:, [<span class="number">2</span>, <span class="number">3</span>]].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">-1</span>].values</span><br></pre></td></tr></table></figure>
<h3 id="拆分数据集"><a href="#拆分数据集" class="headerlink" title="拆分数据集"></a>拆分数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.25</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">sc = StandardScaler()</span><br><span class="line">X_train = sc.fit_transform(X_train)</span><br><span class="line">X_test = sc.fit_transform(X_test)</span><br></pre></td></tr></table></figure>
<h2 id="Step-2-逻辑回归模型"><a href="#Step-2-逻辑回归模型" class="headerlink" title="Step 2: 逻辑回归模型"></a>Step 2: 逻辑回归模型</h2><p>逻辑回归也属于线性模型，两个类别被一条直线分开。导入逻辑回归类之后，生成的分类器用训练集数据进行训练。</p>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">classifier = LogisticRegression()</span><br><span class="line">classifier.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h2 id="Step-3-预测"><a href="#Step-3-预测" class="headerlink" title="Step 3: 预测"></a>Step 3: 预测</h2><h3 id="预测测试集结果"><a href="#预测测试集结果" class="headerlink" title="预测测试集结果"></a>预测测试集结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = classifier.predict(X_test)</span><br></pre></td></tr></table></figure>
<h2 id="Step-4-评估预测效果"><a href="#Step-4-评估预测效果" class="headerlink" title="Step 4: 评估预测效果"></a>Step 4: 评估预测效果</h2><p>通过模型预测测试数据之后，我们将评估模型的准确性。预测正确与否都会包含在混淆矩阵中。</p>
<h3 id="生成混淆矩阵"><a href="#生成混淆矩阵" class="headerlink" title="生成混淆矩阵"></a>生成混淆矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br></pre></td></tr></table></figure>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://smoba.cn/2019/01/17/Day-3-Multiple-Linear-Regression/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Haoxiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深藏功与名">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/17/Day-3-Multiple-Linear-Regression/" class="post-title-link" itemprop="url">Day 003 | 多元线性回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-17 11:42:38" itemprop="dateCreated datePublished" datetime="2019-01-17T11:42:38+08:00">2019-01-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-29 14:10:44" itemprop="dateModified" datetime="2019-01-29T14:10:44+08:00">2019-01-29</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><p>多元线性回归，是通过观测数据训练两个或以上自变量和一个因变量之间关系的线性方程。实现步骤除了模型评估部分外，和简单线性回归很相似。你可以用它来找出哪个因素对预测结果的影响最大，以及不同因素之间的关联。</p>
<p>$y = b_0 + b_1x_1 + b_2x_2 + … + b_nx_n$</p>
<h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><p>下面这些假设前提对一个成功的回归分析非常重要：</p>
<ol>
<li>线性：假定自变量与因变量之间的关系是线性的；</li>
<li>方差齐性：假定不同样本的整体方差不变；</li>
<li>多元正态分布：假定多元回归残差服从正态分布；</li>
<li>无复共线性：假定数据中没有或者很少有复共线性，特征之间相互独立。</li>
</ol>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>过多的变量会降低模型的准确性，尤其当里面存在跟结果无关，或者与其他变量相关的变量。这里有一些选择适合变量的方法：</p>
<ol>
<li>向前选择法</li>
<li>向后选择法</li>
<li>向前向后法</li>
</ol>
<h3 id="虚拟变量"><a href="#虚拟变量" class="headerlink" title="虚拟变量"></a>虚拟变量</h3><p>在多元回归模型中，当遇到非数值类型时，使用分类数据是一个很有效的方法。分类数据是指代表类别的值——固定的、无序的，比如性别（男/女）。在回归模型中，这种值可以用虚拟变量来表示——比如用1/0来表示肯定/否定类型。</p>
<h3 id="虚拟变量陷阱"><a href="#虚拟变量陷阱" class="headerlink" title="虚拟变量陷阱"></a>虚拟变量陷阱</h3><p>虚拟变量陷阱是指两个或以上变量高度相关，简单来说，就是一个变量可以由其他变量推导出来。举一个存在重复类别变量的直观例子：假如我们去掉男性类别，性别也可以通过女性类别来定义（0表示男性，1表示女性），反之亦然。</p>
<p>解决虚拟变量陷阱的方法是，类别变量去掉其中一类：如果总共有m个类别，那么我们在模型中只用m-1个虚拟变量，去掉的那个类别可以当做是参考值。</p>
<h2 id="Step-1-数据处理"><a href="#Step-1-数据处理" class="headerlink" title="Step 1: 数据处理"></a>Step 1: 数据处理</h2><ol>
<li><p>导入库</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入数据集</p>
<p> <a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/datasets/50_Startups.csv" target="_blank" rel="noopener">50_Startups.csv</a></p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">'../datasets/50_Startups.csv'</span>)</span><br><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">-1</span>].values</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查数据丢失</p>
</li>
<li><p>编码分类数据</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"></span><br><span class="line">ct_X = ColumnTransformer([</span><br><span class="line">    (<span class="string">'State'</span>, OneHotEncoder(), [<span class="number">3</span>]),</span><br><span class="line">], remainder=<span class="string">'passthrough'</span>)</span><br><span class="line"></span><br><span class="line">X = ct_X.fit_transform(X)</span><br></pre></td></tr></table></figure>
</li>
<li><p>消除虚拟变量陷阱</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = X[:, <span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
</li>
<li><p>切分数据集</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>特征缩放（由下面用到的线性回归库解决）</p>
</li>
</ol>
<h2 id="Step-2-用测试集训练模型"><a href="#Step-2-用测试集训练模型" class="headerlink" title="Step 2: 用测试集训练模型"></a>Step 2: 用测试集训练模型</h2><p>这一步跟简单线性回归完全一样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor = regressor.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h2 id="Step-3-预测测试集结果"><a href="#Step-3-预测测试集结果" class="headerlink" title="Step 3: 预测测试集结果"></a>Step 3: 预测测试集结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = regressor.predict(X_test)</span><br></pre></td></tr></table></figure>
<p>Done!</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://smoba.cn/2019/01/15/Day-2-Simple-Linear-Regression/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Haoxiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深藏功与名">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/15/Day-2-Simple-Linear-Regression/" class="post-title-link" itemprop="url">Day 002 | 简单线性回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-15 14:27:41" itemprop="dateCreated datePublished" datetime="2019-01-15T14:27:41+08:00">2019-01-15</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-29 14:07:23" itemprop="dateModified" datetime="2019-01-29T14:07:23+08:00">2019-01-29</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简单线性回归"><a href="#简单线性回归" class="headerlink" title="简单线性回归"></a>简单线性回归</h2><h3 id="使用单一特征来预测响应值"><a href="#使用单一特征来预测响应值" class="headerlink" title="使用单一特征来预测响应值"></a>使用单一特征来预测响应值</h3><p>这是一种基于自变量值$X$来预测因变量值$Y$的方法。假设$Y$和$X$是线性相关的，那么我们需要找出一个线性方程，使之尽可能准确的根据$X$值预测出$Y$值。</p>
<h3 id="如何找出最佳拟合线"><a href="#如何找出最佳拟合线" class="headerlink" title="如何找出最佳拟合线"></a>如何找出最佳拟合线</h3><p>在回归模型中，我们将通过找到“最佳拟合线”来最小化预测误差。也就是使得预测值$Y_p$与实际值$Y_i$的距离达到最小。</p>
<p>下面，我们将根据学生学习所花时间，来预测考试成绩：</p>
<p>$Score = b_0 + b_1 * hours$</p>
<h2 id="Step-1-处理数据"><a href="#Step-1-处理数据" class="headerlink" title="Step 1: 处理数据"></a>Step 1: 处理数据</h2><p>按照之前讲过的数据处理步骤：</p>
<ol>
<li><p>导入所需的库</p>
</li>
<li><p>导入数据集</p>
<p><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/datasets/studentscores.csv" target="_blank" rel="noopener">studentscores.csv</a></p>
</li>
<li><p>检查丢失数据</p>
</li>
<li><p>拆分数据集</p>
</li>
<li><p>特征缩放（这里直接用后面简单线性回归模型里的库处理）</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(<span class="string">'../datasets/studentscores.csv'</span>)</span><br><span class="line">X = dataset.iloc[:, :<span class="number">1</span>]</span><br><span class="line">y = dataset.iloc[:, <span class="number">1</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">1</span>/<span class="number">4</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Step-2-通过训练集来训练简单线性回归模型"><a href="#Step-2-通过训练集来训练简单线性回归模型" class="headerlink" title="Step 2: 通过训练集来训练简单线性回归模型"></a>Step 2: 通过训练集来训练简单线性回归模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor = regressor.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h2 id="Step-3-预测结果"><a href="#Step-3-预测结果" class="headerlink" title="Step 3: 预测结果"></a>Step 3: 预测结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = regressor.predict(X_test)</span><br></pre></td></tr></table></figure>
<h2 id="Step-4-可视化"><a href="#Step-4-可视化" class="headerlink" title="Step 4: 可视化"></a>Step 4: 可视化</h2><h3 id="可视化训练集"><a href="#可视化训练集" class="headerlink" title="可视化训练集"></a>可视化训练集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.scatter(X_train, y_train, color=<span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_train, regressor.predict(X_train), color=<span class="string">'blue'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="可视化测试集"><a href="#可视化测试集" class="headerlink" title="可视化测试集"></a>可视化测试集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_test, y_test, color=<span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_test, regressor.predict(X_test), color=<span class="string">'blue'</span>)</span><br></pre></td></tr></table></figure>
<p>Done!</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://smoba.cn/2019/01/14/Day-1-Data-Processing/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Haoxiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深藏功与名">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/14/Day-1-Data-Processing/" class="post-title-link" itemprop="url">Day 001 | 数据处理</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-14 18:58:58" itemprop="dateCreated datePublished" datetime="2019-01-14T18:58:58+08:00">2019-01-14</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-29 13:24:50" itemprop="dateModified" datetime="2019-01-29T13:24:50+08:00">2019-01-29</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>Getting started with machine learning</p>
</blockquote>
<h2 id="Step-1-导入所需的库"><a href="#Step-1-导入所需的库" class="headerlink" title="Step 1: 导入所需的库"></a>Step 1: 导入所需的库</h2><p>下面两个必不可少的库，我们基本每次都会导入：</p>
<ul>
<li>NumPy 包含数学计算函数</li>
<li>Pandas 用于导入和管理数据集</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<h2 id="Step-2-导入数据集"><a href="#Step-2-导入数据集" class="headerlink" title="Step 2: 导入数据集"></a>Step 2: 导入数据集</h2><p>数据集通常是.csv格式。CSV文件以文本形式存储表格数据，文件每一行是一条数据记录。我们使用pandas的read_csv方法将本地CSV文件读取出来生成dataframe，然后从dataframe中生成自变量和因变量的矩阵和向量。</p>
<p><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/datasets/Data.csv" target="_blank" rel="noopener">Data.csv</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = pd.read_csv(<span class="string">'../datasets/Data.csv'</span>)</span><br><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">-1</span>].values</span><br></pre></td></tr></table></figure>
<h2 id="Step-3-处理丢失数据"><a href="#Step-3-处理丢失数据" class="headerlink" title="Step 3: 处理丢失数据"></a>Step 3: 处理丢失数据</h2><p>我们拿到的数据很少是完整的。数据会由于各种各样的原因丢失，我们必须处理这些丢失数据，才能不影响机器学习模型。我们可以用整列的平均值或者中位数替换空值。</p>
<h2 id="Step-4-编码分类数据"><a href="#Step-4-编码分类数据" class="headerlink" title="Step 4: 编码分类数据"></a>Step 4: 编码分类数据</h2><p>分类数据指的是含有标签值而不是数字值的变量，取值范围通常是固定的。例如”Yes”、”No”不能直接用于模型里面数学公式的计算，需要先编码为数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"></span><br><span class="line">ct_X = ColumnTransformer([</span><br><span class="line">    (<span class="string">'country'</span>, OneHotEncoder(), [<span class="number">0</span>]),  <span class="comment"># 编码分类数据</span></span><br><span class="line">    (<span class="string">'others'</span>, SimpleImputer(), slice(<span class="number">1</span>, <span class="number">3</span>))  <span class="comment"># 处理丢失数据</span></span><br><span class="line">])</span><br><span class="line">X = ct_X.fit_transform(X)</span><br><span class="line"></span><br><span class="line">labelencoder_y = LabelEncoder()</span><br><span class="line">y = labelencoder_y.fit_transform(y)</span><br></pre></td></tr></table></figure>
<h2 id="Step-5-将数据集切分成训练集和测试集"><a href="#Step-5-将数据集切分成训练集和测试集" class="headerlink" title="Step 5: 将数据集切分成训练集和测试集"></a>Step 5: 将数据集切分成训练集和测试集</h2><p>我们将数据集分成两部分：</p>
<ul>
<li>训练集：用来训练模型</li>
<li>测试集：用来测试训练好的模型的性能</li>
</ul>
<p>切分比例一般是80 : 20。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Step-6-特征缩放"><a href="#Step-6-特征缩放" class="headerlink" title="Step 6: 特征缩放"></a>Step 6: 特征缩放</h2><p>大部分机器学习算法在计算中使用两点间的欧几里得距离。不同特征在大小、单位和范围构成问题上变化很大，数值较大的特征在计算距离时权值也更大。这个问题可以用特征标准化或者Z-Score标准化解决。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.fit_transform(X_test)</span><br></pre></td></tr></table></figure>
<p>Done!</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://smoba.cn/2019/01/07/Thesis-Structure/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Haoxiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="深藏功与名">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/07/Thesis-Structure/" class="post-title-link" itemprop="url">Thesis Structure - 使用论文结构来撰写技术文章</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-07 21:40:00" itemprop="dateCreated datePublished" datetime="2019-01-07T21:40:00+08:00">2019-01-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-21 14:26:30" itemprop="dateModified" datetime="2019-01-21T14:26:30+08:00">2019-01-21</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/summary/" itemprop="url" rel="index"><span itemprop="name">总结</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="摘要（Abstract）"><a href="#摘要（Abstract）" class="headerlink" title="摘要（Abstract）"></a>摘要（Abstract）</h2><p>整篇文章的概述，大约200-300字，这一步最后写。</p>
<h2 id="引言（Introduction）"><a href="#引言（Introduction）" class="headerlink" title="引言（Introduction）"></a>引言（Introduction）</h2><p>通常会比摘要长，包含以下内容：</p>
<ul>
<li>主题背景</li>
<li>研究现状</li>
<li>现状缺陷和研究目标</li>
<li>其他假设、提纲等</li>
</ul>
<h2 id="文献综述（Literature-review）"><a href="#文献综述（Literature-review）" class="headerlink" title="文献综述（Literature review）"></a>文献综述（Literature review）</h2><p>通常属于引言中的一部分，但也可以是单独的章节。这部分是对主题研究现状的一个评估，主要是提出现状的不足之处，以及本文将如何解决。注意，这里核心点是“评估”。</p>
<h2 id="方案（Methods）"><a href="#方案（Methods）" class="headerlink" title="方案（Methods）"></a>方案（Methods）</h2><p>通常是论文最容易的部分。主要概述：为达到目标，选择的方法和原因。</p>
<h2 id="结果（Results）"><a href="#结果（Results）" class="headerlink" title="结果（Results）"></a>结果（Results）</h2><p>概述在研究问题或假设方面的发现，以图文形式呈现。</p>
<p>结果包含所有的研究事实。通常可以在这里对关键结果的重要性进行简要评论，更多一般的评论最好放在Discussion那一节。也有的时候Results和Discussion是结合在一起的。</p>
<h2 id="讨论（Discussion）"><a href="#讨论（Discussion）" class="headerlink" title="讨论（Discussion）"></a>讨论（Discussion）</h2><p>在Discussion这节：</p>
<ul>
<li>评论你的结果</li>
<li>解释你的结果</li>
<li>在更广泛的背景下解释你的结果；表明哪些结果是预期的或意外的</li>
<li>解释意外情况</li>
</ul>
<p>这里也应将你的具体结果与之前的研究或理论联系起来。你需要指出论文的局限性，列出仍未解决的问题，然后也可以写当前结论和未来的研究。</p>
<h2 id="结论（Conclusions）"><a href="#结论（Conclusions）" class="headerlink" title="结论（Conclusions）"></a>结论（Conclusions）</h2><p>这节非常重要，这是强调你的研究目标已经实现的地方，阐述最重要的成果，列出局限性以及给未来的研究提出建议。</p>
<p>结论可以包含未来的研究方向。</p>
<h2 id="引用（Reference）"><a href="#引用（Reference）" class="headerlink" title="引用（Reference）"></a>引用（Reference）</h2><p>[1] <a href="https://student.unsw.edu.au/thesis-structure" target="_blank" rel="noopener">https://student.unsw.edu.au/thesis-structure</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Haoxiang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/haoxiang2019" title="GitHub &rarr; https://github.com/haoxiang2019" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:haoxiang@tencent.com" title="E-Mail &rarr; mailto:haoxiang@tencent.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Haoxiang</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  




  





  

  

  

  

  
  

  
  
    
      
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

</body>
</html>
